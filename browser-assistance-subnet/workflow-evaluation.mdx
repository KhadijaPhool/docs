---
title: Workflow Evalutaions
description: ''
---

## Workflow Evaluation

Evaluating the Workflow generated by miners is key to have a complete pipeline. This will be the neccesary feedback that will make this subnet generate correct workflows that actually solve the users requests. Eventhough we could use reasoning and logic approaches, we believe the only real way of evaluating a workflow is to actually execute the Workflow in a virtual browser environment.

As you may have noticed, workflows are agnostic in terms of where should those actions/steps be executed. Workflows just describe and state a series of actions from a predetermine finite list of actions that must be executed. This actions depending on the environment that have to be executed will need a different implementation. For example, a simple action of clicking or navigating will take different forms if we are using a tools like Pupetter (Javascript) than when using Seelenium (python). We could even interact with a browser directly using the Cromnium API. 

The execution of this Workflow actions will have to be supervised by an AI proccess that will try to solve any possible error that arrises during the execution and that will eventually evaluate (given defined criteria) how acurate was the workflow. We can combine this with cuantitative metrics that take into account correct actions, errors, total iterations till success etc. 

Here we could choose from different criteria to evaluate. Just giving score to those workflows that are completely correct or to also give score to technically wrong workflows that could be solved in the execution phase by doing little adjustments. This scoring criteria will be key and will dictate how miners operate. All this while keeping validator code light and fast enough to handle lots of miners simulatenously. 